import tensorflow as tf
import numpy as np
import re
import os
from utils import rnn_cell, random_generator, batch_generator

def timegan(ori_data, ori_time, max_val, min_val, max_seq_len, parameters):
  tf.logging.set_verbosity(tf.logging.ERROR)

  # Initialization on the Graph
  tf.reset_default_graph()

  # Network Parameters
  gamma = 1
  z_dim = dim = 6
  no = len(ori_data)
  module = parameters['module']
  work_dir = parameters['work_dir']
  hidden_dim = parameters['hidden_dim']
  num_layer = parameters['num_layer']
  iterations = parameters['iterations']
  batch_size = parameters['batch_size']
  learning_rate = parameters['learning_rate']

  ## Build a RNN networks
  def embedder(X, T):
    """Embedding network between original feature space to latent space.

    Args:
      - X: input time-series features
      - T: input time information

    Returns:
      - H: embeddings
    """
    with tf.variable_scope("embedder", reuse=tf.AUTO_REUSE):
      e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module, hidden_dim) for _ in range(num_layer)])
      e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, X, dtype=tf.float32, sequence_length=T)
      H = tf.contrib.layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)
    return H

  def recovery(H, T):
    """Recovery network from latent space to original space.

    Args:
      - H: latent representation
      - T: input time information

    Returns:
      - X_tilde: recovered data
    """
    with tf.variable_scope("recovery", reuse=tf.AUTO_REUSE):
      r_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module, hidden_dim) for _ in range(num_layer)])
      r_outputs, r_last_states = tf.nn.dynamic_rnn(r_cell, H, dtype=tf.float32, sequence_length=T)
      X_tilde = tf.contrib.layers.fully_connected(r_outputs, dim, activation_fn=tf.nn.sigmoid)
    return X_tilde

  def generator(Z, T):
    """Generator function: Generate time-series data in latent space.

    Args:
      - Z: random variables
      - T: input time information

    Returns:
      - E: generated embedding
    """
    with tf.variable_scope("generator", reuse=tf.AUTO_REUSE):
      e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module, hidden_dim) for _ in range(num_layer)])
      e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, Z, dtype=tf.float32, sequence_length=T)
      E = tf.contrib.layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)
    return E

  def supervisor(H, T):
    """Generate next sequence using the previous sequence.

    Args:
      - H: latent representation
      - T: input time information

    Returns:
      - S: generated sequence based on the latent representations generated by the generator
    """
    with tf.variable_scope("supervisor", reuse=tf.AUTO_REUSE):
      e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module, hidden_dim) for _ in range(num_layer - 1)])
      e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, H, dtype=tf.float32, sequence_length=T)
      S = tf.contrib.layers.fully_connected(e_outputs, hidden_dim, activation_fn=tf.nn.sigmoid)
    return S

  def discriminator(H, T):
    """Discriminate the original and synthetic time-series data.

    Args:
      - H: latent representation
      - T: input time information

    Returns:
      - Y_hat: classification results between original and synthetic time-series
    """
    with tf.variable_scope("discriminator", reuse=tf.AUTO_REUSE):
      d_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module, hidden_dim) for _ in range(num_layer)])
      d_outputs, d_last_states = tf.nn.dynamic_rnn(d_cell, H, dtype=tf.float32, sequence_length=T)
      Y_hat = tf.contrib.layers.fully_connected(d_outputs, 1, activation_fn=None)
    return Y_hat

  def position_loss_fn(H, low, upp):
    logits = tf.math.pow((tf.minimum(0., upp - H) + tf.minimum(0., H - low)), 2)
    return tf.reduce_sum(logits)

  # Input place holders
  X = tf.placeholder(tf.float32, [None, max_seq_len, dim], name="myinput_x")
  Z = tf.placeholder(tf.float32, [None, max_seq_len, z_dim], name="myinput_z")
  T = tf.placeholder(tf.int32, [None], name="myinput_t")

  # Embedder & Recovery
  H = embedder(X, T)
  X_tilde = recovery(H, T)

  # Generator
  E_hat = generator(Z, T)
  H_hat = supervisor(E_hat, T)
  H_hat_supervise = supervisor(H, T)

  # Synthetic data
  X_hat = recovery(H_hat, T)

  # Discriminator
  Y_fake = discriminator(H_hat, T)
  Y_real = discriminator(H, T)
  Y_fake_e = discriminator(E_hat, T)

  # Variables
  e_vars = [v for v in tf.trainable_variables() if v.name.startswith('embedder')]
  r_vars = [v for v in tf.trainable_variables() if v.name.startswith('recovery')]
  g_vars = [v for v in tf.trainable_variables() if v.name.startswith('generator')]
  s_vars = [v for v in tf.trainable_variables() if v.name.startswith('supervisor')]
  d_vars = [v for v in tf.trainable_variables() if v.name.startswith('discriminator')]

  # Discriminator loss
  D_loss_real = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_real), Y_real)
  D_loss_fake = tf.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake), Y_fake)
  D_loss_fake_e = tf.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake_e), Y_fake_e)
  D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e

  # Generator loss
  # 1. Adversarial loss
  G_loss_U = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake), Y_fake)
  G_loss_U_e = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake_e), Y_fake_e)

  # 2. Supervised loss
  X1_loss_pt = position_loss_fn(H_hat_supervise[:, :, 1], 10, 790)
  X2_loss_pt = position_loss_fn(H_hat_supervise[:, :, 3], 10, 790)
  Y1_loss_pt = position_loss_fn(H_hat_supervise[:, :, 2], 10, 790)
  Y2_loss_pt = position_loss_fn(H_hat_supervise[:, :, 4], 10, 790)

  G_loss_S = tf.losses.mean_squared_error(H[:, 1:, :], H_hat_supervise[:, :-1, :])
  G_loss_pt_tot = G_loss_S + X1_loss_pt + X2_loss_pt + Y1_loss_pt + Y2_loss_pt

  # 3. Two Momments
  G_loss_V1 = tf.reduce_mean(
    tf.abs(tf.sqrt(tf.nn.moments(X_hat, [0])[1] + 1e-6) - tf.sqrt(tf.nn.moments(X, [0])[1] + 1e-6)))
  G_loss_V2 = tf.reduce_mean(tf.abs((tf.nn.moments(X_hat, [0])[0]) - (tf.nn.moments(X, [0])[0])))

  G_loss_V = G_loss_V1 + G_loss_V2

  # 4. Summation
  G_loss = G_loss_U + gamma * G_loss_U_e + 100 * tf.sqrt(G_loss_S) + 100 * G_loss_V + G_loss_pt_tot

  E_loss_T0 = tf.losses.mean_squared_error(X, X_tilde)
  E_loss0 = 10 * tf.sqrt(E_loss_T0)
  E_loss = E_loss0 + 0.1 * G_loss_S

  # optimizer
  E0_solver = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(E_loss0, var_list=e_vars + r_vars)
  E_solver = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(E_loss, var_list=e_vars + r_vars)
  D_solver = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(D_loss, var_list=d_vars)
  G_solver = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(G_loss, var_list=g_vars + s_vars)
  GS_solver = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(G_loss_S, var_list=g_vars + s_vars)

  ## TimeGAN training
  sess = tf.Session()
  sess.run(tf.global_variables_initializer())
  saver = tf.train.Saver()

  max_number = '0'
  checkpoint_path = os.path.join(work_dir, 'checkpoints')
  if not os.path.exists(checkpoint_path):
    os.makedirs(checkpoint_path)
  if os.listdir(checkpoint_path):
    # Load Model From Checkpoints, Skip Embedding and Supervised Training
    print('Load Model From Checkpoints')
    checkpoint_files = [file for file in os.listdir(checkpoint_path) if file.endswith('.' + 'index')]
    print("Available checkpoint files: {}".format(checkpoint_files))
    epoch_numbers = [re.search(r'([0-9]*)(?=\.)', file).group() for file in checkpoint_files]
    max_number = max(epoch_numbers)
    print("checkpoint max_number: ", max_number)
    saver.restore(sess,  os.path.join(checkpoint_path, 'ckpt') + max_number)
  else:
    # 1. Embedding network training
    print('Start Embedding Network Training')
    for itt in range(iterations):
      # Set mini-batch
      X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)
      # Train embedder
      _, step_e_loss = sess.run([E0_solver, E_loss_T0], feed_dict={X: X_mb, T: T_mb})
      # Checkpoint
      if itt % 1000 == 0 or itt + 1 == iterations:
        print('step: ' + str(itt+1) + '/' + str(iterations) + ', e_loss: ' + str(np.round(np.sqrt(step_e_loss), 4)))
    print('Finish Embedding Network Training')

    # 2. Training only with supervised loss
    print('Start Training with Supervised Loss Only')
    for itt in range(iterations):
      # Set mini-batch
      X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)
      # Random vector generation
      Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)
      # Train generator
      _, step_g_loss_s = sess.run([GS_solver, G_loss_S], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})
      # Checkpoint
      if itt % 1000 == 0 or itt + 1 == iterations:
        print('step: ' + str(itt+1) + '/' + str(iterations) + ', s_loss: ' + str(np.round(np.sqrt(step_g_loss_s), 4)))
    print('Finish Training with Supervised Loss Only')

  # 3. Joint Training
  print('Start Joint Training')
  for itt in range(int(max_number), iterations):
    # Generator training (twice more than discriminator training)
    for kk in range(2):
      # Set mini-batch
      X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)
      # Random vector generation
      Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)
      # Train generator
      _, step_g_loss_u, step_g_loss_s, step_g_loss_v = sess.run([G_solver, G_loss_U, G_loss_S, G_loss_V],
                                                                feed_dict={Z: Z_mb, X: X_mb, T: T_mb})
      # Train embedder
      _, step_e_loss_t0 = sess.run([E_solver, E_loss_T0], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})

      # Discriminator training
    # Set mini-batch
    X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)
    # Random vector generation
    Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)
    # Check discriminator loss before updating
    check_d_loss = sess.run(D_loss, feed_dict={X: X_mb, T: T_mb, Z: Z_mb})
    # Train discriminator (only when the discriminator does not work well)
    if (check_d_loss > 0.15):
      _, step_d_loss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, T: T_mb, Z: Z_mb})

    # Print multiple checkpoints
    if itt % 1000 == 0 or itt + 1 == iterations:
      print('step: ' + str(itt+1) + '/' + str(iterations) +
            ', d_loss: ' + str(np.round(step_d_loss, 4)) +
            ', g_loss_u: ' + str(np.round(step_g_loss_u, 4)) +
            ', g_loss_s: ' + str(np.round(np.sqrt(step_g_loss_s), 4)) +
            ', g_loss_v: ' + str(np.round(step_g_loss_v, 4)) +
            ', e_loss_t0: ' + str(np.round(np.sqrt(step_e_loss_t0), 4)))

    if itt % 1000 == 0:
      save_path = saver.save(sess, os.path.join(checkpoint_path, 'ckpt'), global_step=itt)
      print("checkpoint no.", itt, "save to", save_path)
  print('Finish Joint Training')

  ## Save Model
  save_path = saver.save(sess, os.path.join(checkpoint_path, 'ckpt'))
  print("Model saved in path: %s" % save_path)

  ## Synthetic data generation
  Z_mb = random_generator(no, z_dim, ori_time, max_seq_len)
  generated_data_curr = sess.run(X_hat, feed_dict={Z: Z_mb, X: ori_data, T: ori_time})
  generated_data = list()

  for i in range(no):
    temp = generated_data_curr[i, :ori_time[i], :]
    generated_data.append(temp * max_val + min_val)

  return generated_data
